{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebeaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.models.detection as tmd\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor as FRP\n",
    "import inspect\n",
    "import torchvision\n",
    "#converting the customised rcnn model\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a47476",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'D:\\\\Drone-Object-Detection\\\\VisDroneDataset\\\\'\n",
    "changed_dir = 'D:\\\\Drone-Object-Detection\\\\VisDroneDataset_changed\\\\'\n",
    "test_dir = 'test\\\\'\n",
    "train_dir = 'train\\\\'\n",
    "val_dir = 'validation\\\\'\n",
    "labels_map = ['Background', 'Pedestrian', 'People', 'Bycicle', 'Car', 'Van', 'Truck',\n",
    "                'Tricycle', 'Awning-tricycle', 'Bus', 'Motor', 'Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50897fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hist(channel_array):\n",
    "    def Sh(h,i):\n",
    "        return sum(h[:i])\n",
    "    h = np.bincount(channel_array.flatten())\n",
    "    H = sum(h)\n",
    "    h = h/H\n",
    "    \n",
    "    for i in range(channel_array.shape[0]):\n",
    "        for j in range(channel_array.shape[1]):\n",
    "            channel_array[i][j] = 255 * Sh(h,channel_array[i][j])\n",
    "                                         \n",
    "    return channel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_histogram_equalization(image):\n",
    "    \n",
    "    b,g,r = image[0].astype(np.uint8),image[1].astype(np.uint8),image[2].astype(np.uint8)\n",
    "    \n",
    "    b_eq = normalize_hist(b)\n",
    "    g_eq = normalize_hist(g)\n",
    "    r_eq = normalize_hist(r)\n",
    "    \n",
    "    equalized_image = cv2.merge((b_eq, g_eq, r_eq))\n",
    "    #equalized_image = np.stack((b_eq, g_eq, r_eq))\n",
    "    return equalized_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf63632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hsv_histogram_equalization(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv_image)\n",
    "    v_eq = cv2.equalizeHist(v)\n",
    "    equalized_hsv = cv2.merge((h, s, v_eq))\n",
    "    equalized_image = cv2.cvtColor(equalized_hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return equalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_croppping(to_save,idx, image_path,bounding_boxes):\n",
    "    to_file=''\n",
    "    for index, box in enumerate(bounding_boxes): \n",
    "        x1_b, y1_b,width , height, a, b, c,d = box\n",
    "        to_file+=f\"{x1_b},{y1_b},{width},{height},{a},{b},{c},{d}\\n\"\n",
    "\n",
    "\n",
    "    opencvImage = cv2.imread(image_path)\n",
    "    cv2.imwrite(f'{to_save}train\\\\smart_images\\\\output_image_not_changed_{idx}.png',opencvImage)\n",
    "    with open(f'{to_save}train\\\\smart_annotations\\\\bounding_boxes_not_changed_{idx}.txt', 'w') as bbox_file:\n",
    "                bbox_file.write(to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ece5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_and_save_boxes(to_save, image_path, txt_path,idx,img_shape=(480,480)):\n",
    "    # Load the image\n",
    "    original_image = Image.open(image_path)   \n",
    "    bounding_boxes=[]\n",
    "    with open(txt_path, 'r') as file:#check whether 0.8 are small\n",
    "        bounding_boxes = np.array([list(map(int, line.split(','))) for line in file.readlines()])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    w_new = img_shape[0]\n",
    "    h_new = img_shape[1]\n",
    "    w,h = original_image.size\n",
    "    # Calculate the number of 480x480 boxes to create\n",
    "    num_boxes_horizontal = w// w_new  # Ceiling division\n",
    "    num_boxes_vertical = h // h_new\n",
    "\n",
    "    # Iterate over boxes and crop the image\n",
    "    for i in range(num_boxes_horizontal):\n",
    "        for j in range(num_boxes_vertical):\n",
    "            # Calculate box coordinates\n",
    "            x1_new = i * w_new\n",
    "            y1_new =  j * h_new\n",
    "            x2_new = (i+1) * w_new\n",
    "            y2_new = (j+1) * h_new\n",
    "            \n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = original_image.crop((x1_new, y1_new, x2_new, y2_new))\n",
    "\n",
    "            # Save the cropped image\n",
    "            #cropped_image.save(f'{to_save}train\\\\images_cropped\\\\output_image_{idx}_{i}_{j}.png')\n",
    "            opencvImage = cv2.cvtColor(np.array(cropped_image), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            to_file=''\n",
    "            bb_new = []\n",
    "            obs_num=0\n",
    "            needed_el = 0\n",
    "            for index, box in enumerate(bounding_boxes):\n",
    "                \n",
    "                x1_b, y1_b,width , height, a, b, c,d = box\n",
    "                x2_b = x1_b + width\n",
    "                y2_b = y1_b + height\n",
    "\n",
    "\n",
    "                if (x1_b > x2_new or y1_b > y2_new) or (x2_b < x1_new or y2_b < y1_new): # no itersection\n",
    "\n",
    "                    continue\n",
    "                else:\n",
    "                    \n",
    "                    x1_b_new = max(0, x1_b - x1_new)\n",
    "                    y1_b_new = max(0, y1_b - y1_new)\n",
    "                    x2_b_new = min(x2_new - x1_new, x2_b - x1_new)\n",
    "                    y2_b_new = min(y2_new - y1_new, y2_b - y1_new)#bug here\n",
    "                    \n",
    "                    to_file+=f\"{x1_b_new},{y1_b_new},{x2_b_new - x1_b_new},{y2_b_new - y1_b_new},{a},{b},{c},{d}\\n\"\n",
    "                    obs_num+=1\n",
    "                    #needed_el += (b==6 or b==7 or b==8 or b==9)\n",
    "                    if(x1_b_new < 0 or y1_b_new < 0 or  x2_b_new > 480 or y2_b_new > 480):\n",
    "                        print(to_file,txt_path)\n",
    "                        \n",
    "                    \n",
    "                #labels = bb_new[:,5]\n",
    "                #small = [i  for i in bb_new if ]\n",
    "\n",
    "\n",
    "            if(obs_num < 10) :\n",
    "                continue\n",
    "\n",
    "            with open(f'{to_save}train\\\\smart_annotations\\\\bounding_boxes_{idx}_{i}_{j}.txt', 'w') as bbox_file:\n",
    "                bbox_file.write(to_file)\n",
    "            cv2.imwrite(f'{to_save}train\\\\smart_images\\\\output_image_{idx}_{i}_{j}.png', opencvImage)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "# Example usage\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "txt_path = 'path/to/your/bounding_boxes.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89156d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(from_directory,to_directory, shape):\n",
    "    images = os.listdir(from_directory+'images\\\\')\n",
    "    annotations = os.listdir(from_directory+'annotations\\\\')\n",
    "   \n",
    "    for idx,(img_dir,ann_dir) in enumerate(zip(images, annotations)):\n",
    "        \n",
    "        image = cv2.imread(from_directory+'images\\\\'+ img_dir)  \n",
    "        \n",
    "        #image = sharp_img(image)\n",
    "        \n",
    "        img_width = image.shape[1]\n",
    "        img_height = image.shape[0]\n",
    "        \n",
    "        new_width = shape[0]\n",
    "        new_height = shape[1]\n",
    "        if shape == (-1,-1):\n",
    "            image = cv2.resize(image,(shape[0],shape[1]),interpolation = cv2.INTER_AREA)\n",
    "        image = hsv_histogram_equalization(image)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(from_directory+'annotations\\\\'+ann_dir,'r') as f_f,open(to_directory+'annotations\\\\'+ann_dir,'w') as f_t:\n",
    "            new_f = ''\n",
    "            for line in f_f:\n",
    "                boxes = []\n",
    "                data = line.split(',')#strip\n",
    "                #print(list(map(float, data[:5])))\n",
    "                box_left, box_top, box_width, box_height,smth1, category,smth2,smth3 = map(float, data)\n",
    "               \n",
    "                box_width = 1 if box_width < 1 else box_width\n",
    "                box_height = 1 if box_height < 1 else box_height\n",
    "                \n",
    "                box_right = box_left + box_width\n",
    "                \n",
    "                box_bottom = box_top + box_height\n",
    "                #rescale box            \n",
    "                box_left = (box_left/img_width) * new_width\n",
    "                box_right = (box_right/img_width) * new_width\n",
    "                box_top = (box_top/img_height) * new_height\n",
    "                box_bottom = (box_bottom/img_height) * new_height\n",
    "                boxes += [box_left,box_top,box_right, box_bottom]\n",
    "                #print(boxes)\n",
    "                new_f += ''.join([str(int(c))+',' for c in boxes])\n",
    "                new_f+=str(int(smth1))+','\n",
    "                new_f += str(int(category))+','\n",
    "                new_f += str(int(smth2))+','\n",
    "                new_f += str(int(smth3))+'\\n'\n",
    "                \n",
    "                \n",
    "            f_t.write(new_f)\n",
    "            \n",
    "        \n",
    "        if(idx % 100==0):\n",
    "            print(f'imf N {idx} written')\n",
    "        #print('img written')\n",
    "        cv2.imwrite(to_directory+'images\\\\'+ img_dir,image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_images(root_dir+train_dir,changed_dir+train_dir ,(480,480))\n",
    "def crop():\n",
    "    images = os.listdir(root_dir+train_dir+'images\\\\')\n",
    "    annotations = os.listdir(root_dir+train_dir+'annotations\\\\')\n",
    "\n",
    "    for idx,(img_dir,ann_dir) in enumerate(zip(images, annotations)):\n",
    "        crop_and_save_boxes(changed_dir, root_dir+train_dir+'images\\\\'+img_dir,root_dir+train_dir+'annotations\\\\'+ann_dir,\\\n",
    "                            idx,img_shape = (480,480))\n",
    "        if(idx % 100 == 0):\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f32ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73763507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, path, img_shape):\n",
    "        \n",
    "        self.root = path\n",
    "        self.width = img_shape[1]\n",
    "        self.height = img_shape[0]\n",
    "        self.images = os.listdir(path+'images\\\\')\n",
    "        self.annotations = os.listdir(path+'annotations\\\\')\n",
    "        self.length = len(self.images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.root+'images\\\\'+self.images[idx])\n",
    "        image = cv2.imread(self.root+'images\\\\'+self.images[idx])\n",
    "        #print(image.shape)\n",
    "        img_width = image.shape[1]\n",
    "        img_height = image.shape[0]     #right?\n",
    "        \n",
    "        image  =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        if ((self.width,self.height) != (-1,-1)):\n",
    "            image = cv2.resize(image,(self.width,self.height),interpolation = cv2.INTER_AREA)\n",
    "        else:\n",
    "            self.width = img_width\n",
    "            self.height = img_height\n",
    "            \n",
    "        image = image.transpose(2,0,1)\n",
    "        image = (image/255.0)\n",
    "        \n",
    "        \n",
    "        #bbox_left, bbox_top, bbox_width, bbox_height, category = map(float, data[:5])\n",
    "        target = self.annotations[idx]    #*.txt file       \n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(self.root+'annotations\\\\'+target) as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')#strip\n",
    "                #print(list(map(float, data[:5])))\n",
    "                box_left, box_top, box_width, box_height,smth, category = map(float, data[:6])\n",
    "                \n",
    "                box_width = 1 if box_width < 1 else box_width\n",
    "                box_height = 1 if box_height < 1 else box_height\n",
    "                \n",
    "                box_right = box_left + box_width\n",
    "                box_bottom = box_top + box_height\n",
    "                #rescale box            \n",
    "                box_left = (box_left/img_width) * self.width\n",
    "                box_right = (box_right/img_width) * self.width\n",
    "                box_top = (box_top/img_height) * self.height\n",
    "                box_bottom = (box_bottom/img_height) * self.height\n",
    "                boxes.append((box_left,box_top,box_right, box_bottom))\n",
    "#                 for b in boxes:\n",
    "#                     for bb in b:\n",
    "#                         if bb > 480 or bb < 0:\n",
    "#                             print(b,'\\n',bb)\n",
    "                labels.append(int(category))\n",
    "                \n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        #print(boxes)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor((idx))\n",
    "        \n",
    "        return image, target # np.array, dict of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisDroneDataset(changed_dir+train_dir,(480,480))\n",
    "val_dataset = VisDroneDataset(root_dir+val_dir,(-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23293e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "\n",
    "    return (images, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56551681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir('D:\\\\torch_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19824791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b891431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ssd():\n",
    "    model = ssdlite320_mobilenet_v3_large(num_classes=12)\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c650c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_faster_rcnn(num_classes):\n",
    "    # Load a pre-trained  R-CNN model \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad=False\n",
    "        \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model():\n",
    "    with open('model.pkl','rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        return  model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d969e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_faster_rcnn(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b2262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "def train_one_epoch(model, train_loader, optimizer):\n",
    "    total_loss=0\n",
    "    n=0\n",
    "    prev_time = time.time()\n",
    "    for data, target in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(len(data))\n",
    "        data = torch.as_tensor(np.array(data),dtype=torch.float32).to(device)\n",
    "        target = [{k: v.to(device) for k, v in t.items()} for t in target]\n",
    "        \n",
    "        #print(target)\n",
    "        loss_dict = model(data, target)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss+=losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if(n % 100 == 0):\n",
    "            print('loss', total_loss/(n+1))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "        \n",
    "        n+=1\n",
    "        del data, target\n",
    "        #model = model.to(torch.device('cpu'))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #model = model.to(device)\n",
    "    \n",
    "    print(total_loss/n,'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155d137",
   "metadata": {},
   "source": [
    "if(n % 1000 == 0):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(data)\n",
    "                \n",
    "            nms_preds=[]\n",
    "            \n",
    "            for pred in preds:\n",
    "                nms_pred = non_max_suppression_git(pred)\n",
    "                nms_preds.append(nms_pred)\n",
    "                \n",
    "            map_val = calc_mAP(nms_pred,annotations)['map'].item()\n",
    "            print(map_val)\n",
    "            model.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 50\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.000005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8d189",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "print(len(params))\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7be9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbb3df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_one_epoch(model,train_loader,optimizer)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model_trained_on_cropped.pkl', 'wb') as f:\n",
    "#    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model_trained_on_cropped.pkl', 'rb') as f:\n",
    "#    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95296509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_best_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/obss/sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.utils.cv import read_image\n",
    "#from IPython.display import Image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fa0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou_batch(boxes_a: np.ndarray, boxes_b: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    def box_area(box):\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area_a = box_area(boxes_a.T)\n",
    "    area_b = box_area(boxes_b.T)\n",
    "\n",
    "    top_left = np.maximum(boxes_a[:, None, :2], boxes_b[:, :2])\n",
    "    bottom_right = np.minimum(boxes_a[:, None, 2:], boxes_b[:, 2:])\n",
    "\n",
    "    area_inter = np.prod(\n",
    "    \tnp.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n",
    "        \n",
    "    return area_inter / (area_a[:, None] + area_b - area_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(predictions: dict, iou_threshold: float = 0.9) -> np.ndarray:\n",
    "    #print(predictions)\n",
    "    predictions = torch.cat((predictions['boxes'] , predictions['scores'].unsqueeze(dim=1),\\\n",
    "                             predictions['labels'].unsqueeze(dim=1)),dim=1) .detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    sort_index = np.flip(predictions[:, 4].argsort())\n",
    "    \n",
    "    rows,columns = predictions.shape\n",
    "    \n",
    "    predictions = predictions[sort_index]\n",
    "\n",
    "    boxes = predictions[:, :4]\n",
    "    categories = predictions[:, 5]\n",
    "    ious = box_iou_batch(boxes, boxes)\n",
    "    ious = ious - np.eye(rows)\n",
    "\n",
    "    keep = np.ones(rows, dtype=bool)\n",
    "\n",
    "    for index, (iou, category) in enumerate(zip(ious, categories)):\n",
    "        if not keep[index]:\n",
    "            continue\n",
    "\n",
    "        condition = (iou > iou_threshold) & (categories == category)\n",
    "        keep = keep & ~condition\n",
    "\n",
    "    return predictions[keep[sort_index.argsort()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b20167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8403ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = ['Background', 'Pedestrian', 'People', 'Bycicle', 'Car', 'Van', 'Truck',\n",
    "                'Tricycle', 'Awning-tricycle', 'Bus', 'Motor', 'Others']\n",
    "def draw_bounding_boxes(img: np.ndarray, boxes: np.ndarray,labels: np.ndarray,scores: np.ndarray, ax):   # image - tensor, others - numpy\n",
    "    \n",
    "    ax.imshow(img.detach().cpu().numpy().transpose(1,2,0))\n",
    "    ax.set_title('title')\n",
    "    threshold = 0.2\n",
    "    for box,label,score in zip(boxes,labels.astype(np.uint8),scores):\n",
    "        #print(label)\n",
    "        if(score < threshold):\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "                x1,\n",
    "                y1,\n",
    "                f\"{labels_map[label]}: {(np.round(score*100,1))}%\",\n",
    "                verticalalignment='top',\n",
    "                color='white',\n",
    "                fontsize=8,\n",
    "                bbox={'facecolor': 'red', 'alpha': 0.7, 'pad': 1}\n",
    "            )\n",
    "    #plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd612a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "\n",
    "def non_max_suppression_git(preds: dict, iou_threshold=0.5, score_threshold=0.2):\n",
    "    \n",
    "    boxes = preds['boxes']\n",
    "    scores = preds['scores']\n",
    "    labels = preds['labels']\n",
    "\n",
    "    # Filter out predictions below the score_threshold\n",
    "    keep_idx = scores > score_threshold\n",
    "    boxes = boxes[keep_idx]\n",
    "    scores = scores[keep_idx]\n",
    "    labels = labels[keep_idx]\n",
    "\n",
    "    # Perform NMS and get the indices of the remaining predictions\n",
    "    keep_indices = nms(boxes, scores, iou_threshold)\n",
    "\n",
    "    # Filter out the predictions using the keep_indices\n",
    "    nms_boxes = boxes[keep_indices]\n",
    "    nms_scores = scores[keep_indices]\n",
    "    nms_labels = labels[keep_indices]\n",
    "\n",
    "    # Create a dictionary containing the filtered predictions\n",
    "    nms_preds = {\n",
    "        'boxes': nms_boxes,\n",
    "        'scores': nms_scores,\n",
    "        'labels': nms_labels\n",
    "    }\n",
    "\n",
    "    return nms_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69799d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions(imgs: list, preds: list, mode: str, i):#preds - list of dictionaries of tensors\n",
    "    fig, ax = plt.subplots(1,len(imgs), figsize=(40,40))\n",
    "    for idx,(img,ann) in enumerate(zip(imgs, preds)):\n",
    "        #print(ann)\n",
    "        annotations_new = []\n",
    "        annotations_new.append(torch.cat((ann['boxes'] ,\\\n",
    "                                           (ann['scores']).unsqueeze(dim=1) if mode=='pred' else torch.ones(len(ann['boxes'])).\\\n",
    "                                          unsqueeze(dim=1),\\\n",
    "                                           ann['labels'].unsqueeze(dim=1)),dim=1).detach().cpu().numpy())\n",
    "        ann = np.array(annotations_new[0])\n",
    "        #print(ann)\n",
    "        draw_bounding_boxes(img,ann[:,0:4],ann[:,5],ann[:,4],ax[idx] if len(imgs) != 1 else ax)\n",
    "        \n",
    "    if(i!=-1):\n",
    "        fig.savefig(f'D:\\\\Drone-Object-Detection\\\\Results\\\\Images\\\\img{i}.png')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_cuda_with_cpu(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.cpu()\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: replace_cuda_with_cpu(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_cuda_with_cpu(item) for item in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_cpu(preds):\n",
    "    new_l = []\n",
    "    for dic in preds:\n",
    "        new_l.append(replace_cuda_with_cpu(dic))\n",
    "        \n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_dict_to_arr(dict):\n",
    "    annotations_new = []\n",
    "    for ann in dict:\n",
    "        \n",
    "        annotations_new+=(torch.cat((ann['boxes'] ,\\\n",
    "                                           torch.ones(len(ann['boxes'])).unsqueeze(dim=1).to(device),\\\n",
    "                                           ann['labels'].unsqueeze(dim=1)),dim=1).detach().cpu().numpy())\n",
    "        \n",
    "    return annotations_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0332177",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file = open('Results\\\\mAPs\\\\file.txt','w')\n",
    "map_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mAP(preds: list,target: list):\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    metric.update(preds, annotations)\n",
    "    #map_file.write(str(metric.compute())+'\\n')\n",
    "    #print(metric.compute())\n",
    "    return (metric.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fe260",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file = open('Results\\\\mAPs\\\\file.txt','a')\n",
    "#path to img - 'Results\\\\imgs\\\\img(i).jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac66f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='torchvision',\n",
    "    model=model,\n",
    "    confidence_threshold=0.5,\n",
    "    image_size=1080,\n",
    "    device=\"cuda:0\", \n",
    "    load_at_init=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de266343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_x1y1wh_to_x1y1x2y2(box: list)->list:\n",
    "    \n",
    "    box[2] = box[0] + box[2]\n",
    "    box[3] = box[1] + box[3]\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_common_dict(list_of_dict):\n",
    "    #print(list_of_dict)\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for dic in list_of_dict:\n",
    "        #print(dic['bbox'])\n",
    "        boxes.append(from_x1y1wh_to_x1y1x2y2(dic['bbox']))\n",
    "        labels.append(dic['category_id'])##not sure\n",
    "        scores.append(dic['score'])\n",
    "        \n",
    "    return {'scores':torch.tensor(scores),'labels':torch.tensor(labels), 'boxes':torch.tensor(boxes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3271bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_SAHI(batch_imgs, detection_model):\n",
    "    results = []\n",
    "    for img in batch_imgs:\n",
    "        result = get_sliced_prediction(\n",
    "            Image.fromarray(np.moveaxis((img * 255).astype(np.uint8), [0,1,2],[2,0,1])),#2 1 0\n",
    "            detection_model,\n",
    "            slice_height = 480,\n",
    "            slice_width = 480,\n",
    "            overlap_height_ratio = 0.2,\n",
    "            overlap_width_ratio = 0.2 \n",
    "        ) # returns x1y1wh\n",
    "        results.append(to_common_dict(result.to_coco_annotations()))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_without_SAHI(batch_imgs, model):\n",
    "    batch_imgs = [torch.from_numpy(img).float().to(device) for img in batch_imgs]\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch_imgs)#tensor\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ce358",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_for_map = DataLoader(val_dataset, batch_size=1, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f1fd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maps = \"\"\n",
    "c = 0\n",
    "total_map_sahi = 0\n",
    "total_map_no_sahi = 0\n",
    "results=[]\n",
    "for i, (batch_imgs, annotations) in enumerate(val_loader_for_map):\n",
    "    \n",
    "    results_sahi = predict_with_SAHI(batch_imgs, detection_model)\n",
    "    results_no_sahi = predict_without_SAHI(batch_imgs,model)\n",
    "    \n",
    "    preds_sahi = replace_cuda_with_cpu(results_sahi) \n",
    "    preds_no_sahi = replace_cuda_with_cpu(results_no_sahi) \n",
    "    \n",
    "    nms_preds_sahi = []\n",
    "    for pred_sahi in preds_sahi:\n",
    "        nms_pred_sahi = non_max_suppression_git(pred_sahi)       \n",
    "        nms_preds_sahi.append(nms_pred_sahi)\n",
    "        \n",
    "    \n",
    "    nms_preds_no_sahi = []\n",
    "    for pred_no_sahi in preds_no_sahi:\n",
    "        nms_pred_no_sahi = non_max_suppression_git(pred_no_sahi)       \n",
    "        nms_preds_no_sahi.append(nms_pred_no_sahi)\n",
    "    #batch_imgs = [torch.from_numpy(img).float().to(device) for img in batch_imgs]#tensor\n",
    "    #draw_predictions(batch_imgs,nms_preds,'pred',1)#list of dictionaries of tensors\n",
    "    #draw_predictions(batch_imgs,annotations,'tar',-1)\n",
    "\n",
    "    #print(nms_preds)\n",
    "    total_map_sahi +=calc_mAP(nms_preds_sahi,annotations)['map'].item()\n",
    "    total_map_no_sahi +=calc_mAP(nms_preds_no_sahi,annotations)['map'].item()\n",
    "    c+=1\n",
    "    \n",
    "    if(i == 5):\n",
    "        break\n",
    "    if(i % 20 == 0):\n",
    "        print(f'{i}, With SAHI map: {total_map_sahi / c}')\n",
    "        print(f'{i}, Without SAHI map: {total_map_no_sahi / c}')\n",
    "        \n",
    "print(f'With SAHI map: {total_map_sahi / c}')\n",
    "print(f'Without SAHI map: {total_map_no_sahi / c}')\n",
    "#Trouble SAHI with dataset\n",
    "#D:\\Drone-Object-Detection\\\\VisDroneDataset\\\\validation\\\\images\\\\0000001_04527_d_0000008.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07538d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BETTER PEDESTRIANS WORSE CARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6477753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([1,2,3] in [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548a0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0036d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_with_SAHI([1], detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ab133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
