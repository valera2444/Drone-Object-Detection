{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebeaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.models.detection as tmd\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor as FRP\n",
    "import inspect\n",
    "import torchvision\n",
    "#converting the customised rcnn model\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a47476",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'D:\\\\Drone-Object-Detection\\\\VisDroneDataset\\\\'\n",
    "changed_dir = 'D:\\\\Drone-Object-Detection\\\\VisDroneDataset_changed\\\\'\n",
    "test_dir = 'test\\\\'\n",
    "train_dir = 'train\\\\'\n",
    "val_dir = 'validation\\\\'\n",
    "labels_map = ['Background', 'Pedestrian', 'People', 'Bycicle', 'Car', 'Van', 'Truck',\n",
    "                'Tricycle', 'Awning-tricycle', 'Bus', 'Motor', 'Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50897fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hist(channel_array):\n",
    "    def Sh(h,i):\n",
    "        return sum(h[:i])\n",
    "    h = np.bincount(channel_array.flatten())\n",
    "    H = sum(h)\n",
    "    h = h/H\n",
    "    \n",
    "    for i in range(channel_array.shape[0]):\n",
    "        for j in range(channel_array.shape[1]):\n",
    "            channel_array[i][j] = 255 * Sh(h,channel_array[i][j])\n",
    "                                         \n",
    "    return channel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_histogram_equalization(image):\n",
    "    \n",
    "    b,g,r = image[0].astype(np.uint8),image[1].astype(np.uint8),image[2].astype(np.uint8)\n",
    "    \n",
    "    b_eq = normalize_hist(b)\n",
    "    g_eq = normalize_hist(g)\n",
    "    r_eq = normalize_hist(r)\n",
    "    \n",
    "    equalized_image = cv2.merge((b_eq, g_eq, r_eq))\n",
    "    #equalized_image = np.stack((b_eq, g_eq, r_eq))\n",
    "    return equalized_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf63632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hsv_histogram_equalization(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv_image)\n",
    "    v_eq = cv2.equalizeHist(v)\n",
    "    equalized_hsv = cv2.merge((h, s, v_eq))\n",
    "    equalized_image = cv2.cvtColor(equalized_hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return equalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_croppping(to_save,idx, image_path,bounding_boxes):\n",
    "    to_file=''\n",
    "    for index, box in enumerate(bounding_boxes): \n",
    "        x1_b, y1_b,width , height, a, b, c,d = box\n",
    "        to_file+=f\"{x1_b},{y1_b},{width},{height},{a},{b},{c},{d}\\n\"\n",
    "\n",
    "\n",
    "    opencvImage = cv2.imread(image_path)\n",
    "    cv2.imwrite(f'{to_save}train\\\\smart_images\\\\output_image_not_changed_{idx}.png',opencvImage)\n",
    "    with open(f'{to_save}train\\\\smart_annotations\\\\bounding_boxes_not_changed_{idx}.txt', 'w') as bbox_file:\n",
    "                bbox_file.write(to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ece5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_and_save_boxes(to_save, image_path, txt_path,idx,img_shape=(480,480)):\n",
    "    # Load the image\n",
    "    original_image = Image.open(image_path)   \n",
    "    bounding_boxes=[]\n",
    "    with open(txt_path, 'r') as file:#check whether 0.8 are small\n",
    "        bounding_boxes = np.array([list(map(int, line.split(','))) for line in file.readlines()])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    w_new = img_shape[0]\n",
    "    h_new = img_shape[1]\n",
    "    w,h = original_image.size\n",
    "    # Calculate the number of 480x480 boxes to create\n",
    "    num_boxes_horizontal = w// w_new  # Ceiling division\n",
    "    num_boxes_vertical = h // h_new\n",
    "\n",
    "    # Iterate over boxes and crop the image\n",
    "    for i in range(num_boxes_horizontal):\n",
    "        for j in range(num_boxes_vertical):\n",
    "            # Calculate box coordinates\n",
    "            x1_new = i * w_new\n",
    "            y1_new =  j * h_new\n",
    "            x2_new = (i+1) * w_new\n",
    "            y2_new = (j+1) * h_new\n",
    "            \n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = original_image.crop((x1_new, y1_new, x2_new, y2_new))\n",
    "\n",
    "            # Save the cropped image\n",
    "            #cropped_image.save(f'{to_save}train\\\\images_cropped\\\\output_image_{idx}_{i}_{j}.png')\n",
    "            opencvImage = cv2.cvtColor(np.array(cropped_image), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            to_file=''\n",
    "            bb_new = []\n",
    "            obs_num=0\n",
    "            needed_el = 0\n",
    "            for index, box in enumerate(bounding_boxes):\n",
    "                \n",
    "                x1_b, y1_b,width , height, a, b, c,d = box\n",
    "                x2_b = x1_b + width\n",
    "                y2_b = y1_b + height\n",
    "\n",
    "\n",
    "                if (x1_b > x2_new or y1_b > y2_new) or (x2_b < x1_new or y2_b < y1_new): # no itersection\n",
    "\n",
    "                    continue\n",
    "                else:\n",
    "                    \n",
    "                    x1_b_new = max(0, x1_b - x1_new)\n",
    "                    y1_b_new = max(0, y1_b - y1_new)\n",
    "                    x2_b_new = min(x2_new - x1_new, x2_b - x1_new)\n",
    "                    y2_b_new = min(y2_new - y1_new, y2_b - y1_new)#bug here\n",
    "                    \n",
    "                    to_file+=f\"{x1_b_new},{y1_b_new},{x2_b_new - x1_b_new},{y2_b_new - y1_b_new},{a},{b},{c},{d}\\n\"\n",
    "                    obs_num+=1\n",
    "                    #needed_el += (b==6 or b==7 or b==8 or b==9)\n",
    "                    if(x1_b_new < 0 or y1_b_new < 0 or  x2_b_new > 480 or y2_b_new > 480):\n",
    "                        print(to_file,txt_path)\n",
    "                        \n",
    "                    \n",
    "                #labels = bb_new[:,5]\n",
    "                #small = [i  for i in bb_new if ]\n",
    "\n",
    "\n",
    "            if(obs_num < 10) :\n",
    "                continue\n",
    "\n",
    "            with open(f'{to_save}train\\\\smart_annotations\\\\bounding_boxes_{idx}_{i}_{j}.txt', 'w') as bbox_file:\n",
    "                bbox_file.write(to_file)\n",
    "            cv2.imwrite(f'{to_save}train\\\\smart_images\\\\output_image_{idx}_{i}_{j}.png', opencvImage)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "# Example usage\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "txt_path = 'path/to/your/bounding_boxes.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89156d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(from_directory,to_directory, shape):\n",
    "    images = os.listdir(from_directory+'images\\\\')\n",
    "    annotations = os.listdir(from_directory+'annotations\\\\')\n",
    "   \n",
    "    for idx,(img_dir,ann_dir) in enumerate(zip(images, annotations)):\n",
    "        \n",
    "        image = cv2.imread(from_directory+'images\\\\'+ img_dir)  \n",
    "        \n",
    "        #image = sharp_img(image)\n",
    "        \n",
    "        img_width = image.shape[1]\n",
    "        img_height = image.shape[0]\n",
    "        \n",
    "        new_width = shape[0]\n",
    "        new_height = shape[1]\n",
    "        if shape == (-1,-1):\n",
    "            image = cv2.resize(image,(shape[0],shape[1]),interpolation = cv2.INTER_AREA)\n",
    "        image = hsv_histogram_equalization(image)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(from_directory+'annotations\\\\'+ann_dir,'r') as f_f,open(to_directory+'annotations\\\\'+ann_dir,'w') as f_t:\n",
    "            new_f = ''\n",
    "            for line in f_f:\n",
    "                boxes = []\n",
    "                data = line.split(',')#strip\n",
    "                #print(list(map(float, data[:5])))\n",
    "                box_left, box_top, box_width, box_height,smth1, category,smth2,smth3 = map(float, data)\n",
    "               \n",
    "                box_width = 1 if box_width < 1 else box_width\n",
    "                box_height = 1 if box_height < 1 else box_height\n",
    "                \n",
    "                box_right = box_left + box_width\n",
    "                \n",
    "                box_bottom = box_top + box_height\n",
    "                #rescale box            \n",
    "                box_left = (box_left/img_width) * new_width\n",
    "                box_right = (box_right/img_width) * new_width\n",
    "                box_top = (box_top/img_height) * new_height\n",
    "                box_bottom = (box_bottom/img_height) * new_height\n",
    "                boxes += [box_left,box_top,box_right, box_bottom]\n",
    "                #print(boxes)\n",
    "                new_f += ''.join([str(int(c))+',' for c in boxes])\n",
    "                new_f+=str(int(smth1))+','\n",
    "                new_f += str(int(category))+','\n",
    "                new_f += str(int(smth2))+','\n",
    "                new_f += str(int(smth3))+'\\n'\n",
    "                \n",
    "                \n",
    "            f_t.write(new_f)\n",
    "            \n",
    "        \n",
    "        if(idx % 100==0):\n",
    "            print(f'imf N {idx} written')\n",
    "        #print('img written')\n",
    "        cv2.imwrite(to_directory+'images\\\\'+ img_dir,image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_images(root_dir+train_dir,changed_dir+train_dir ,(480,480))\n",
    "def crop():\n",
    "    images = os.listdir(root_dir+train_dir+'images\\\\')\n",
    "    annotations = os.listdir(root_dir+train_dir+'annotations\\\\')\n",
    "\n",
    "    for idx,(img_dir,ann_dir) in enumerate(zip(images, annotations)):\n",
    "        crop_and_save_boxes(changed_dir, root_dir+train_dir+'images\\\\'+img_dir,root_dir+train_dir+'annotations\\\\'+ann_dir,\\\n",
    "                            idx,img_shape = (480,480))\n",
    "        if(idx % 100 == 0):\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f32ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73763507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, path, img_shape):\n",
    "        \n",
    "        self.root = path\n",
    "        self.width = img_shape[1]\n",
    "        self.height = img_shape[0]\n",
    "        self.images = os.listdir(path+'images\\\\')\n",
    "        self.annotations = os.listdir(path+'annotations\\\\')\n",
    "        self.length = len(self.images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.root+'images\\\\'+self.images[idx])\n",
    "        image = cv2.imread(self.root+'images\\\\'+self.images[idx])\n",
    "        #print(image.shape)\n",
    "        img_width = image.shape[1]\n",
    "        img_height = image.shape[0]     #right?\n",
    "        \n",
    "        image  =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        if ((self.width,self.height) != (-1,-1)):\n",
    "            image = cv2.resize(image,(self.width,self.height),interpolation = cv2.INTER_AREA)\n",
    "        else:\n",
    "            self.width = img_width\n",
    "            self.height = img_height\n",
    "            \n",
    "        image = image.transpose(2,0,1)\n",
    "        image = (image/255.0)\n",
    "        \n",
    "        \n",
    "        #bbox_left, bbox_top, bbox_width, bbox_height, category = map(float, data[:5])\n",
    "        target = self.annotations[idx]    #*.txt file       \n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(self.root+'annotations\\\\'+target) as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')#strip\n",
    "                #print(list(map(float, data[:5])))\n",
    "                box_left, box_top, box_width, box_height,smth, category = map(float, data[:6])\n",
    "                \n",
    "                box_width = 1 if box_width < 1 else box_width\n",
    "                box_height = 1 if box_height < 1 else box_height\n",
    "                \n",
    "                box_right = box_left + box_width\n",
    "                box_bottom = box_top + box_height\n",
    "                #rescale box            \n",
    "                box_left = (box_left/img_width) * self.width\n",
    "                box_right = (box_right/img_width) * self.width\n",
    "                box_top = (box_top/img_height) * self.height\n",
    "                box_bottom = (box_bottom/img_height) * self.height\n",
    "                boxes.append((box_left,box_top,box_right, box_bottom))\n",
    "#                 for b in boxes:\n",
    "#                     for bb in b:\n",
    "#                         if bb > 480 or bb < 0:\n",
    "#                             print(b,'\\n',bb)\n",
    "                labels.append(int(category))\n",
    "                \n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        #print(boxes)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor((idx))\n",
    "        \n",
    "        return image, target # np.array, dict of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisDroneDataset(changed_dir+train_dir,(480,480))\n",
    "val_dataset = VisDroneDataset(root_dir+val_dir,(-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23293e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "\n",
    "    return (images, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56551681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir('D:\\\\torch_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19824791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ssd():\n",
    "    model = ssdlite320_mobilenet_v3_large(num_classes=12)\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c650c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_faster_rcnn(num_classes):\n",
    "    # Load a pre-trained  R-CNN model \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad=False\n",
    "        \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model():\n",
    "    with open('model.pkl','rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        return  model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d969e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_faster_rcnn(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "def train_one_epoch(model, train_loader, optimizer):\n",
    "    total_loss=0\n",
    "    n=0\n",
    "    prev_time = time.time()\n",
    "    for data, target in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(len(data))\n",
    "        data = torch.as_tensor(np.array(data),dtype=torch.float32).to(device)\n",
    "        target = [{k: v.to(device) for k, v in t.items()} for t in target]\n",
    "        \n",
    "        #print(target)\n",
    "        loss_dict = model(data, target)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss+=losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if(n % 100 == 0):\n",
    "            print('loss', total_loss/(n+1))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "        \n",
    "        n+=1\n",
    "        del data, target\n",
    "        #model = model.to(torch.device('cpu'))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #model = model.to(device)\n",
    "    \n",
    "    print(total_loss/n,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 50\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.000005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbb3df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_one_epoch(model,train_loader,optimizer)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model_trained_on_cropped.pkl', 'wb') as f:\n",
    "#    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
